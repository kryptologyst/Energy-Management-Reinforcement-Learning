# Energy Management Reinforcement Learning

Research-ready implementation of reinforcement learning for energy management systems, featuring state-of-the-art algorithms, comprehensive evaluation, and interactive visualization.

## ‚ö†Ô∏è IMPORTANT SAFETY DISCLAIMER

**WARNING: This is a research/educational simulation. NOT FOR PRODUCTION USE in real energy management systems.**

Real energy management systems require:
- Extensive safety validation and testing
- Regulatory compliance and certification
- Redundant safety systems and fail-safes
- Professional engineering oversight
- Integration with certified control systems

This implementation is for research, education, and algorithm development only.

## Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/kryptologyst/Energy-Management-Reinforcement-Learning.git
cd Energy-Management-Reinforcement-Learning
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Train an agent:
```bash
python scripts/train.py --agent-type sac --num-episodes 2000
```

4. Run the interactive demo:
```bash
streamlit run demo/streamlit_demo.py
```

## üìÅ Project Structure

```
rl-energy-management/
‚îú‚îÄ‚îÄ src/                          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/               # RL algorithms (DQN, SAC)
‚îÇ   ‚îú‚îÄ‚îÄ environments/             # Energy management environment
‚îÇ   ‚îú‚îÄ‚îÄ training/                 # Training and evaluation pipelines
‚îÇ   ‚îî‚îÄ‚îÄ utils/                    # Configuration and utilities
‚îú‚îÄ‚îÄ configs/                      # Configuration files
‚îú‚îÄ‚îÄ scripts/                      # Training and evaluation scripts
‚îú‚îÄ‚îÄ demo/                         # Interactive Streamlit demo
‚îú‚îÄ‚îÄ tests/                        # Unit tests
‚îú‚îÄ‚îÄ notebooks/                    # Jupyter notebooks for analysis
‚îú‚îÄ‚îÄ assets/                       # Generated plots and results
‚îú‚îÄ‚îÄ data/                         # Datasets and logs
‚îî‚îÄ‚îÄ docs/                         # Documentation
```

## Architecture

### Environment
The `EnergyManagementEnv` simulates a microgrid with:
- **Energy Storage**: Configurable battery system
- **Renewable Generation**: Solar and wind energy sources
- **Variable Demand**: Time-varying energy consumption patterns
- **Grid Connection**: Dynamic electricity pricing

**State Space**: `[storage_level, demand, generation, price_deviation, hour_of_day]`
**Action Space**: `[charge_rate, discharge_rate, grid_buy_rate, grid_sell_rate]`

### Algorithms
- **DQN**: Deep Q-Network with experience replay and target network
- **SAC**: Soft Actor-Critic for continuous control with automatic temperature tuning

### Training Pipeline
- Comprehensive logging and checkpointing
- Statistical evaluation with confidence intervals
- Device fallback (CUDA ‚Üí MPS ‚Üí CPU)
- Reproducible experiments with deterministic seeding

## Usage Examples

### Basic Training

```python
from src.environments.energy_management import EnergyManagementEnv
from src.algorithms.modern_rl import SACAgent
from src.training.trainer import EnergyRLTrainer

# Create environment
env = EnergyManagementEnv(
    max_storage_capacity=100.0,
    max_demand=50.0,
    max_generation=80.0
)

# Create agent
agent = SACAgent(
    state_dim=env.observation_space.shape[0],
    action_dim=env.action_space.shape[0]
)

# Train
trainer = EnergyRLTrainer(env, agent, config)
metrics = trainer.train(num_episodes=2000)
```

### Evaluation

```python
from src.training.trainer import EnergyRLEvaluator

evaluator = EnergyRLEvaluator(env)
metrics = evaluator.evaluate_agent(agent, num_episodes=100)

print(f"Mean Reward: {metrics['rewards']['mean']:.2f}")
print(f"Success Rate: {metrics['success_rate']:.2%}")
```

### Configuration

```yaml
# configs/custom.yaml
env:
  max_storage_capacity: 150.0
  max_demand: 75.0
  price_variability: 0.4

agent:
  type: sac
  lr: 1e-4
  gamma: 0.99

training:
  num_episodes: 5000
  eval_frequency: 200
```

## Evaluation Metrics

### Learning Metrics
- **Average Return**: Mean episode reward ¬± 95% CI
- **Success Rate**: Percentage of episodes with positive reward
- **Sample Efficiency**: Steps to reach performance threshold
- **Training Stability**: Reward variance and convergence

### Control Metrics
- **Energy Cost**: Total cost of energy operations
- **Storage Utilization**: Average battery usage
- **Constraint Violations**: Safety constraint breaches
- **Grid Interaction**: Buy/sell patterns

### Safety Metrics
- **Constraint Satisfaction**: Percentage of safe operations
- **Violation Frequency**: Violations per 1000 steps
- **Risk Measures**: CVaR and tail event analysis

## Research Features

### Reproducibility
- Deterministic seeding for all random components
- Configurable reproducibility settings
- Comprehensive logging of hyperparameters

### Evaluation Suite
- Statistical significance testing
- Confidence interval reporting
- Ablation study support
- Multi-agent comparison tools

### Visualization
- Interactive Streamlit dashboard
- Real-time policy visualization
- Performance metric plots
- Action distribution analysis

## üõ†Ô∏è Advanced Usage

### Custom Environments

```python
class CustomEnergyEnv(EnergyManagementEnv):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Add custom modifications
    
    def step(self, action):
        # Custom reward function
        reward = self._custom_reward(action)
        return super().step(action)
```

### Hyperparameter Tuning

```python
# Use Optuna for hyperparameter optimization
import optuna

def objective(trial):
    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)
    gamma = trial.suggest_float('gamma', 0.9, 0.999)
    
    agent = SACAgent(lr=lr, gamma=gamma)
    trainer = EnergyRLTrainer(env, agent, config)
    metrics = trainer.train(num_episodes=1000)
    
    return metrics['eval_rewards'][-1]

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

### Multi-Agent Comparison

```python
agents = {
    'SAC': sac_agent,
    'DQN': dqn_agent,
    'Random': random_agent
}

evaluator = EnergyRLEvaluator(env)
leaderboard = evaluator.compare_agents(agents, num_episodes=50)
```

## Expected Results

### Performance Benchmarks
- **SAC**: Typically achieves 80-90% success rate
- **DQN**: Achieves 70-85% success rate
- **Sample Efficiency**: SAC converges in ~1000 episodes
- **Energy Cost**: 20-40% reduction compared to random policy

### Training Time
- **CPU**: ~2-4 hours for 2000 episodes
- **GPU**: ~30-60 minutes for 2000 episodes
- **Memory**: ~2-4 GB RAM usage

## Testing

Run the test suite:

```bash
pytest tests/ -v
```

Run specific tests:

```bash
pytest tests/test_environment.py -v
pytest tests/test_algorithms.py -v
```

## Documentation

- **API Reference**: See docstrings in source code
- **Examples**: Check `notebooks/` directory
- **Configuration**: See `configs/` directory
- **Tutorials**: Interactive demo in `demo/`

## Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Built on PyTorch and Gymnasium
- Inspired by energy management research
- Uses modern RL algorithm implementations

## Support

For questions and support:
- Create an issue on GitHub
- Check the documentation
- Review the example notebooks

---

**Remember**: This is a research simulation. Always validate algorithms thoroughly before considering any real-world applications in energy systems.
# Energy-Management-Reinforcement-Learning
